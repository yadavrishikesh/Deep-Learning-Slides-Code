# Reading Group on Deep Learning and its Application in Mathematics and Statistics

**Total Duration:** One whole semester (16 weeks)  
**Frequency of Meetings:** Once a week, 2 hours each  
**Attendees:** SMSS faculty and PhD students (not mandatory, only for interested ones)

---

## Motivation & Relevance

Deep learning is no longer just a tool for computer scientists; it has become a powerful tool for solving problems across mathematics, statistics, science, and engineering.  

For **PhD students and faculty of SMSS**, this group is designed not only to teach architectures like **CNNs, RNNs, Transformers, and PINNs**, but also to **bridge deep learning with rigorous mathematical thinking**.

### For PhD Students:
- Learn how deep learning connects with concepts you already know, such as **linear algebra, probability, optimization, and differential equations**, so the transition will feel natural rather than foreign.  
- Every module will come with **structured notes, reproducible code, and research references** so that what you learn here can directly support your thesis or future publications.

### For Faculty:
- Many of us want to learn about deep learning, but due to busy schedules, we never get time to explore these topics.  
- With **organized slides, documented code, and curated reading materials**, faculty members can get a solid overview of these fast-growing fields by spending a maximum of **two hours a week**.  

ðŸ‘‰ In short, this is not only a reading group; it is an **academic accelerator** that equips the next generation of mathematicians and scientists with tools that are rapidly growing and defining the frontier of research worldwide.

---

## Curriculum (Weekly Plan)

### ðŸ“Œ Month 1 â€“ Foundations of Deep Learning (DL)  
**Goal:** Build the math, ML background, and neural network basics.

**Week 1:** Background and Motivation on Deep Learning *(by Rishi)*  
- Historical Development of DL and Current Relevance  
- Basic Tools: Python (NumPy, TensorFlow basics, plotting)  
- Probability and Statistics  
- Classification & Regression Overview (Supervised Learning)  

**Week 2:** Machine Learning (Statistics) Refresher *(by Rishi)*  
- Regression (linear, logistic)  
- Gradient Descent Optimization  
- Train-test split, evaluation metrics (accuracy, precision, recall, F1)  

**Week 3:** Neural Networks Basics *(by Rishi)*  
- Perceptron, Multilayer Perceptron (MLP)  
- Backpropagation & Loss Functions  
- Build NN from scratch (NumPy)  

**Week 4:** Practical Deep Learning Setup *(by Rishi)*  
- Optimizers (SGD, Adam, RMSProp)  
- Overfitting/Underfitting, Dropout, Batch Norm  
- **Project:** MNIST Handwritten Digit Classification  

---

### ðŸ“Œ Month 2 â€“ Core Architectures  
**Goal:** Cover foundational architectures every DL practitioner must know.

**Week 5:** Convolutional Neural Networks (CNNs) *(by two PhD students)*  
- Convolution, Pooling, Filters  
- LeNet, AlexNet, VGG  
- Train a CNN on CIFAR-10  

**Week 6:** Advanced CNNs *(by two PhD students)*  
- ResNet (skip connections), DenseNet  
- EfficientNet Overview  
- Applications: Image Classification, Feature Extraction  

**Week 7:** Sequence Models (RNNs) *(by two PhD students)*  
- Recurrent Neural Networks  
- Vanishing Gradient Problem  
- Simple Text Classification with RNN  

**Week 8:** LSTMs & GRUs *(by two PhD students)*  
- Long Short-Term Memory (LSTM)  
- Gated Recurrent Units (GRUs)  
- Applications: Sentiment Analysis, Time Series  

---

### ðŸ“Œ Month 3 â€“ Transformers & Generative Models  
**Goal:** Learn modern architectures powering NLP & Generative AI.

**Week 9:** Attention Mechanisms *(by two PhD students)*  
- Seq2Seq, Encoder-Decoder  
- Scaled Dot-Product Attention  

**Week 10:** Transformers *(Dr. Tanmay Kayal)*  
- Transformer Architecture (Self-Attention, Positional Encoding)  
- Applications in NLP (Translation, Summarization)  
- BERT & GPT Overview  

**Week 11:** Generative Models *(by two PhD students)*  
- Autoencoders & Variational Autoencoders (VAE)  
- Generative Adversarial Networks (GANs)  
- Applications: Image Synthesis, Data Augmentation  

**Week 12:** Vision Transformers (ViT) *(by two PhD students)*  
- Transformers for Vision Tasks  
- Comparison with CNNs  
- Train a small ViT model on CIFAR-10  

---

### ðŸ“Œ Month 4 â€“ Specialized & Widely Used Applications  
**Goal:** Explore advanced and domain-specific models that are widely impactful.

**Week 13:** Physics-Informed Neural Networks (PINNs) *(by two PhD students)*  
- Introduction to Scientific Machine Learning  
- Solving PDEs with Neural Networks  
- Applications in Physics, Climate Modeling  

**Week 14:** Graph Neural Networks (GNNs) *(by two PhD students)*  
- Basics of Graph Theory in ML  
- Graph Convolutional Networks (GCN)  
- Applications: Social Networks, Molecules, Recommendation Systems  

**Week 15:** Large Language Models (LLMs) *(by two PhD students)*  
- GPT Family, BERT, T5 Overview  
- Hugging Face Transformers Hands-on  
- Fine-tuning LLMs for Custom Tasks  

**Week 16:** Reinforcement Learning & Beyond *(by two PhD students)*  
- Basics of RL (Q-learning, Policy Gradients)  
- Deep Reinforcement Learning (DQN)  
- Applications in Robotics, Games, Control Systems  

---
