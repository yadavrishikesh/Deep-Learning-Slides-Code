# Reading Group on Deep Learning and its Application in Mathematics

**Total Duration:** One whole semester extended to **5–6 months (22 weeks)**  
**Frequency of Meetings:** Once a week, 2 hours each  
**Attendees:** SMSS faculty and PhD students (not mandatory, only for interested ones)

---

## Motivation & Relevance

Deep learning is no longer just a tool for computer scientists; it has become a powerful tool for solving problems across mathematics, statistics, science, and engineering.  

For **PhD students and faculty of SMSS**, this group is designed not only to teach architectures like **CNNs, RNNs, Transformers, PINNs**, but also to **bridge deep learning with rigorous mathematical thinking**.

### For PhD Students:
- Learn how deep learning connects with concepts you already know, such as **linear algebra, probability, optimization, and differential equations**, so the transition will feel natural rather than foreign.  
- Every module will come with **structured notes, reproducible code, and research references** so that what you learn here can directly support your thesis or future publications.

### For Faculty:
- Many of us want to learn about deep learning, but due to busy schedules, we never get time to explore these topics.  
- With **organized slides, documented code, and curated reading materials**, faculty members can get a solid overview of these fast-growing fields by spending a maximum of **two hours a week**.  

 In short, this is not only a reading group; it is an **academic accelerator** that equips the next generation of mathematicians and scientists with tools that are rapidly growing and defining the frontier of research worldwide.

---

## Curriculum (Weekly Plan)

###  Month 1 – Foundations of Deep Learning (DL)  
**Goal:** Build the math, ML background, and neural network basics.

**Week 1:** Background and Motivation on Deep Learning  
- Historical Development of DL and Current Relevance  
- Basic Tools: Python (NumPy, TensorFlow basics, plotting)  
- Probability and Statistics  
- Classification & Regression Overview (Supervised Learning)  

**Week 2:** Machine Learning (Statistics) Refresher  
- Regression (linear, logistic)  
- Gradient Descent Optimization  
- Train-test split, evaluation metrics (accuracy, precision, recall, F1)  

**Week 3:** Neural Networks Basics  
- Perceptron, Multilayer Perceptron (MLP)  
- Backpropagation & Loss Functions  
- Build NN from scratch (NumPy)  

**Week 4:** Practical Deep Learning Setup  
- Optimizers (SGD, Adam, RMSProp)  
- Overfitting/Underfitting, Dropout, Batch Norm  
- **Project:** MNIST Handwritten Digit Classification  

---

###  Month 2 – Core Architectures  
**Goal:** Cover foundational architectures every DL practitioner must know.

**Week 5:** Convolutional Neural Networks (CNNs)  
- Convolution, Pooling, Filters  
- LeNet, AlexNet, VGG  
- Train a CNN on CIFAR-10  

**Week 6:** Advanced CNNs  
- ResNet (skip connections), DenseNet  
- EfficientNet Overview  
- Applications: Image Classification, Feature Extraction  

**Week 7:** Sequence Models (RNNs)  
- Recurrent Neural Networks  
- Vanishing Gradient Problem  
- Simple Text Classification with RNN  

**Week 8:** LSTMs & GRUs  
- Long Short-Term Memory (LSTM)  
- Gated Recurrent Units (GRUs)  
- Applications: Sentiment Analysis, Time Series  

---

###  Month 3 – Transformers & Generative Models  
**Goal:** Learn modern architectures powering NLP & Generative AI.

**Week 9:** Attention Mechanisms  
- Seq2Seq, Encoder-Decoder  
- Scaled Dot-Product Attention  

**Week 10:** Transformers  
- Transformer Architecture (Self-Attention, Positional Encoding)  
- Applications in NLP (Translation, Summarization)  
- BERT & GPT Overview  

**Week 11:** Generative Models  
- Autoencoders & Variational Autoencoders (VAE)  
- Generative Adversarial Networks (GANs)  
- Applications: Image Synthesis, Data Augmentation  

**Week 12:** Vision Transformers (ViT)  
- Transformers for Vision Tasks  
- Comparison with CNNs  
- Train a small ViT model on CIFAR-10  

---

###  Month 4 – Specialized & Widely Used Applications  
**Goal:** Explore advanced and domain-specific models that are widely impactful.

**Week 13:** Physics-Informed Neural Networks (PINNs)  
- Introduction to Scientific Machine Learning  
- Solving PDEs with Neural Networks  
- Applications in Physics, Climate Modeling  

**Week 14:** Graph Neural Networks (GNNs)  
- Basics of Graph Theory in ML  
- Graph Convolutional Networks (GCN)  
- Applications: Social Networks, Molecules, Recommendation Systems  

---

###  Month 5 – Generative AI & Explainability  
**Goal:** Cover cutting-edge areas shaping AI research & applications.

**Week 15:** Large Language Models (LLMs)  
- GPT family, BERT, T5  
- Hugging Face Transformers Hands-on  
- Fine-tuning LLMs for domain-specific tasks  

**Week 16:** Generative AI in Practice  
- Diffusion Models (e.g., Stable Diffusion)  
- Advanced GANs & VAEs for text, audio, images  
- AI for Creative Applications (art, music, video synthesis)  

**Week 17:** Explainable AI (XAI)  
- Why Interpretability Matters  
- SHAP, LIME, Attention Visualization, Saliency Maps  
- Case Studies: Healthcare, Finance, Climate  

**Week 18:** Robustness, Fairness & Ethics in AI  
- Adversarial Attacks & Defenses  
- Bias & Fairness in Machine Learning  
- Responsible AI Practices in Academia & Industry  

---

### Month 6 – Reinforcement Learning & Beyond  
**Goal:** Round off with cutting-edge applications & open research areas.

**Week 19:** Reinforcement Learning Foundations  
- Q-learning, Policy Gradients  
- Deep Q-Networks (DQN)  

**Week 20:** Advanced RL Applications  
- Robotics, Games, Control Systems  
- Multi-Agent RL  

**Week 21:** AI for Science & Mathematics  
- Symbolic Regression, Theorem Proving  
- Applications in PDE Solving, Optimization, Discovery Science  

**Week 22:** Capstone Presentations  
- Each participant/group presents a mini-project  
- Example: Train & interpret a DL model for your PhD/faculty domain  

---

### Foundational Texts (Weeks 1–4)
- **Deep Learning** — Ian Goodfellow, Yoshua Bengio, Aaron Courville (MIT Press, 2016) → *gold-standard reference for fundamentals*  
- **Pattern Recognition and Machine Learning** — Christopher Bishop (Springer, 2006) → *for ML/statistical foundations*  
- **Mathematics for Machine Learning** — Marc Deisenroth, A. Faisal, C. Ong (Cambridge, 2020) → *math background (linear algebra, probability, optimization)*  

###  Core Architectures (Weeks 5–8)
- **Neural Networks and Deep Learning** — Michael Nielsen (free online book) → *introductory intuition for NNs*  
- **Dive into Deep Learning** — Aston Zhang et al. (open-source book, 2023) → *hands-on code-rich resource*  
- **Backpropagation Paper:** Rumelhart, Hinton & Williams (1986) — *landmark paper introducing backpropagation*  

###  Transformers & Generative Models (Weeks 9–12)
- **Attention Is All You Need** — Vaswani et al. (2017) → *landmark paper introducing Transformers*  
- **The Illustrated Transformer** — Jay Alammar (blog) → *intuitive visual explanation*  
- **Generative Deep Learning** — David Foster (O’Reilly, 2nd ed., 2022) → *great for GANs, VAEs, diffusion models*  

###  Specialized Models (Weeks 13–14)
- **PINNs Paper:** Raissi, Perdikaris & Karniadakis (2019) — *Physics-Informed Neural Networks for PDEs*  
- **Graph Representation Learning** — William L. Hamilton (Morgan & Claypool, 2020) → *solid intro to GNNs*  
- **Graph Neural Networks: A Review of Methods and Applications** — Wu et al. (arXiv, 2020) → *survey paper*  

###  Generative AI, Explainability & Ethics (Weeks 15–18)
- **Deep Generative Models** — Jakub Langr & Vladimir Bok (Packt, 2019) → *GANs, VAEs, diffusion basics*  
- **Diffusion Models Beat GANs?** — Ho et al. (2020, “Denoising Diffusion Probabilistic Models”) → *landmark diffusion model paper*  
- **Interpretable Machine Learning** — Christoph Molnar (free online book, 2022) → *best resource for XAI*  
- **Responsible AI** — Virginia Dignum (Springer, 2019) → *ethical and societal perspective*  

###  Reinforcement Learning & Beyond (Weeks 19–22)
- **Reinforcement Learning: An Introduction** — Sutton & Barto (2nd ed., 2018) → *the RL “bible”*  
- **Playing Atari with Deep Reinforcement Learning** — Mnih et al. (2013, DeepMind) → *landmark paper introducing DQN*  
- **AI for Science Report** — National Academies (2023, free online) → *overview of AI for physics, chemistry, math*  


