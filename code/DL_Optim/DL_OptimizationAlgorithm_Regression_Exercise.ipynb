{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCtmscihk9SeQkYWdWnz7I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yadavrishikesh/Deep-Learning-Slides-Code/blob/main/code/DL_Optim/DL_OptimizationAlgorithm_Regression_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Training Linear Regression with Custom Loss Functions\n",
        "\n",
        "In this exercise, you will train a **linear regression model** on a synthetic dataset and experiment with **different loss functions**.  \n",
        "\n",
        "The goal is to understand how the **choice of loss function affects the training process** and the resulting model.\n",
        "\n",
        "## 1. Dataset\n",
        "\n",
        "We generate a simple 2D dataset where:\n",
        "\n",
        "- $y = 3 \\cdot x_1 - 2 \\cdot x_2 + \\text{noise}$\n",
        "- Students will fit a linear model:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2\n",
        "$$\n",
        "\n",
        "where $w_0$ is the bias and $w_1, w_2$ are the weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "wBdhedbBBfTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate Synthetic Linear Dataset"
      ],
      "metadata": {
        "id": "ZRNHmlh0B8nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate data\n",
        "n_samples = 200\n",
        "X1 = np.random.rand(n_samples,1)*10\n",
        "X2 = np.random.rand(n_samples,1)*5\n",
        "noise = np.random.randn(n_samples,1)\n",
        "\n",
        "y = 3*X1 - 2*X2 + noise\n",
        "X = np.hstack([np.ones((n_samples,1)), X1, X2])  # add bias term\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X1, y, label='y vs x1', alpha=0.5)\n",
        "plt.scatter(X2, y, label='y vs x2', alpha=0.5)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target y')\n",
        "plt.title('Synthetic Linear Regression Data')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vTmNDMkFB49v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define Loss Functions"
      ],
      "metadata": {
        "id": "eTHaARuLBkaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE\n",
        "def mse_loss(y, y_hat):\n",
        "    return np.mean((y - y_hat)**2)\n",
        "\n",
        "def mse_grad(X, y, y_hat):\n",
        "    return -2 * X.T @ (y - y_hat) / X.shape[0]\n",
        "\n",
        "# MAE\n",
        "def mae_loss(y, y_hat):\n",
        "    return np.mean(np.abs(y - y_hat))\n",
        "\n",
        "def mae_grad(X, y, y_hat):\n",
        "    return - X.T @ np.sign(y - y_hat) / X.shape[0]\n",
        "\n",
        "# Huber Loss\n",
        "def huber_loss(y, y_hat, delta=1.0):\n",
        "    e = y - y_hat\n",
        "    mask = np.abs(e) <= delta\n",
        "    loss = np.where(mask, 0.5 * e**2, delta * (np.abs(e) - 0.5*delta))\n",
        "    return np.mean(loss)\n",
        "\n",
        "def huber_grad(X, y, y_hat, delta=1.0):\n",
        "    e = y - y_hat\n",
        "    mask = np.abs(e) <= delta\n",
        "    grad = np.where(mask, -e, -delta*np.sign(e))\n",
        "    return X.T @ grad / X.shape[0]\n"
      ],
      "metadata": {
        "id": "6_kV8YO9CajJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Optimization Algorithms"
      ],
      "metadata": {
        "id": "IvCVU5zjCdxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent\n",
        "def gradient_descent(X, y, loss_func, grad_func, lr=0.01, epochs=100):\n",
        "    w = np.zeros((X.shape[1],1))\n",
        "    loss_history = []\n",
        "    start_time = time.time()\n",
        "    for _ in range(epochs):\n",
        "        y_hat = X @ w\n",
        "        loss_history.append(loss_func(y, y_hat))\n",
        "        grad = grad_func(X, y, y_hat)\n",
        "        w -= lr * grad\n",
        "    end_time = time.time()\n",
        "    return w, loss_history, end_time - start_time\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "def sgd(X, y, loss_func, grad_func, lr=0.01, epochs=100):\n",
        "    w = np.zeros((X.shape[1],1))\n",
        "    loss_history = []\n",
        "    n_samples = X.shape[0]\n",
        "    start_time = time.time()\n",
        "    for _ in range(epochs):\n",
        "        indices = np.random.permutation(n_samples)\n",
        "        X_shuf, y_shuf = X[indices], y[indices]\n",
        "        for i in range(n_samples):\n",
        "            xi = X_shuf[i].reshape(1,-1)\n",
        "            yi = y_shuf[i].reshape(-1,1)\n",
        "            y_hat = xi @ w\n",
        "            grad = grad_func(xi, yi, y_hat)\n",
        "            w -= lr * grad\n",
        "        loss_history.append(loss_func(y, X @ w))\n",
        "    end_time = time.time()\n",
        "    return w, loss_history, end_time - start_time\n",
        "\n",
        "# Adam Optimizer\n",
        "def adam(X, y, loss_func, grad_func, lr=0.01, epochs=100, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "    w = np.zeros((X.shape[1],1))\n",
        "    m = np.zeros_like(w)\n",
        "    v = np.zeros_like(w)\n",
        "    loss_history = []\n",
        "    start_time = time.time()\n",
        "    for t in range(1, epochs+1):\n",
        "        y_hat = X @ w\n",
        "        loss_history.append(loss_func(y, y_hat))\n",
        "        grad = grad_func(X, y, y_hat)\n",
        "        m = beta1*m + (1-beta1)*grad\n",
        "        v = beta2*v + (1-beta2)*(grad**2)\n",
        "        m_hat = m / (1 - beta1**t)\n",
        "        v_hat = v / (1 - beta2**t)\n",
        "        w -= lr * m_hat / (np.sqrt(v_hat) + eps)\n",
        "    end_time = time.time()\n",
        "    return w, loss_history, end_time - start_time\n"
      ],
      "metadata": {
        "id": "6Y-vsYIaCgDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train All Optimizers with MSE\n"
      ],
      "metadata": {
        "id": "8wOhBLjwCi_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "# Gradient Descent\n",
        "w_gd, loss_gd, time_gd = gradient_descent(X, y, mse_loss, mse_grad, lr, epochs)\n",
        "\n",
        "# SGD\n",
        "w_sgd, loss_sgd, time_sgd = sgd(X, y, mse_loss, mse_grad, lr, epochs)\n",
        "\n",
        "# Adam\n",
        "w_adam, loss_adam, time_adam = adam(X, y, mse_loss, mse_grad, lr, epochs)\n",
        "\n",
        "print(\"Execution Times (seconds):\")\n",
        "print(\"GD:\", round(time_gd,4))\n",
        "print(\"SGD:\", round(time_sgd,4))\n",
        "print(\"Adam:\", round(time_adam,4))\n"
      ],
      "metadata": {
        "id": "rZ1bVNYDCklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Convergence Plot\n"
      ],
      "metadata": {
        "id": "sepIBOckCnKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(loss_gd, label=f'GD ({time_gd:.4f}s)')\n",
        "plt.plot(loss_sgd, label=f'SGD ({time_sgd:.4f}s)')\n",
        "plt.plot(loss_adam, label=f'Adam ({time_adam:.4f}s)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Optimizer Convergence with MSE')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6ssCn3ooCokE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Predicted vs True Values\n"
      ],
      "metadata": {
        "id": "khH0NYOqCqFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(y, X @ w_gd, alpha=0.5, label='GD')\n",
        "plt.scatter(y, X @ w_sgd, alpha=0.5, label='SGD')\n",
        "plt.scatter(y, X @ w_adam, alpha=0.5, label='Adam')\n",
        "plt.xlabel('True y')\n",
        "plt.ylabel('Predicted y_hat')\n",
        "plt.title('Predicted vs True Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AczuUInHCsSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Extension Tasks for Students\n",
        "\n",
        "1. Repeat training with **MAE** or **Huber loss**.  \n",
        "2. Compare **convergence**, **final weights**, and **execution time**.  \n",
        "3. Discuss which **loss + optimizer combination** performs best and why.  \n",
        "4. Optionally, add **outliers** to the dataset and see how Huber loss behaves compared to MSE.\n"
      ],
      "metadata": {
        "id": "azfqlCGLCuQ5"
      }
    }
  ]
}