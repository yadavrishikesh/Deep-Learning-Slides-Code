{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSl+DxEY20aSZckt/DErmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yadavrishikesh/Deep-Learning-Slides-Code/blob/main/code/DL_OptimizationAlgorithm_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Training Linear Regression with Custom Loss Functions\n",
        "\n",
        "In this exercise, you will train a **linear regression model** on a synthetic dataset and experiment with **different loss functions**.  \n",
        "\n",
        "The goal is to understand how the **choice of loss function affects the training process** and the resulting model.\n",
        "\n",
        "## 1. Dataset\n",
        "\n",
        "We generate a simple 2D dataset where:\n",
        "\n",
        "- $y = 3 x_1 - 2 x_2 + noise$\n",
        "- Students will try to fit a linear model $\\hat{y} = w_0 + w_1 x_1 + w_2 x_2$\n"
      ],
      "metadata": {
        "id": "SyoN-89I20Ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate synthetic data\n",
        "n_samples = 200\n",
        "X1 = np.random.rand(n_samples,1)*10\n",
        "X2 = np.random.rand(n_samples,1)*5\n",
        "noise = np.random.randn(n_samples,1)\n",
        "\n",
        "y = 3*X1 - 2*X2 + noise\n",
        "X = np.hstack([np.ones((n_samples,1)), X1, X2])  # Add bias term\n",
        "\n",
        "# Plot the data (projection)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.scatter(X1, y, label='y vs x1', alpha=0.5)\n",
        "plt.scatter(X2, y, label='y vs x2', alpha=0.5)\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('Target y')\n",
        "plt.title('Synthetic Linear Regression Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YxDRetoa2-Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define Linear Regression Model\n",
        "\n",
        "The predicted output is:\n",
        "\n",
        "$$\n",
        "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2\n",
        "$$\n",
        "\n",
        "where $w_0$ is the bias, and $w_1, w_2$ are the weights."
      ],
      "metadata": {
        "id": "PnhwlJ0M3NQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Custom Loss Function\n",
        "\n",
        "Instead of always using **Mean Squared Error (MSE)**, we can define a **generalized loss function**:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell(y_i - \\hat{y}_i)\n",
        "$$\n",
        "\n",
        "Where \\(\\ell(\\cdot)\\) is a **loss function**. Examples:\n",
        "\n",
        "1. **MSE (standard)**: \\(\\ell(e) = e^2\\)  \n",
        "2. **MAE (Mean Absolute Error)**: \\(\\ell(e) = |e|\\)  \n",
        "3. **Huber Loss**:\n",
        "\n",
        "$$\n",
        "\\ell_\\delta(e) =\n",
        "\\begin{cases}\n",
        "\\frac{1}{2} e^2 & \\text{if } |e| \\leq \\delta \\\\\n",
        "\\delta (|e| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "> **Task for students**: implement MSE first, then MAE or Huber, and observe how the convergence and weights change.\n"
      ],
      "metadata": {
        "id": "2u7-xmVR3R9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Gradient Computation\n",
        "\n",
        "For linear regression, the **gradient of the loss w.r.t. weights** is:\n",
        "\n",
        "- **MSE**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} = -\\frac{2}{n} X^T (y - \\hat{y})\n",
        "$$\n",
        "\n",
        "- **MAE**:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\mathcal{L}}{\\partial w} = -\\frac{1}{n} X^T \\text{sign}(y - \\hat{y})\n",
        "$$\n",
        "\n",
        "> Students can implement the gradient for any custom loss function they choose.\n"
      ],
      "metadata": {
        "id": "jKUla0cz3XrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Optimization Algorithm\n",
        "\n",
        "Students will implement one or more of:\n",
        "\n",
        "- Gradient Descent (full-batch)\n",
        "- Stochastic Gradient Descent\n",
        "- Adam optimizer  \n",
        "\n",
        "The training steps:\n",
        "\n",
        "1. Initialize weights $w = [0, 0, 0]$\n",
        "2. Compute predicted outputs $\\hat{y} = X w$\n",
        "3. Compute loss using the chosen loss function\n",
        "4. Compute gradient\n",
        "5. Update weights: $w -= lr * gradient$\n",
        "6. Record loss at each epoch\n",
        "7. Repeat for `epochs`\n"
      ],
      "metadata": {
        "id": "u78S1j_d3bYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Visualization Tasks\n",
        "\n",
        "After training:\n",
        "\n",
        "1. **Plot convergence**: loss vs epochs for different loss functions or optimizers.\n",
        "2. **Compare final weights** with ground truth $[w_0, w_1, w_2] = [0, 3, -2]$.\n",
        "3. **Plot predicted vs true values**: scatter plot of `$y$` vs `\\hat{y}`.\n",
        "4. Optionally, **plot decision plane** in 3D using `matplotlib` for visual inspection.\n"
      ],
      "metadata": {
        "id": "h5Rvt1_A3dwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Advanced Extension\n",
        "\n",
        "- Try **Huber Loss** and observe its effect on **robustness to outliers**.\n",
        "- Compare **GD vs SGD vs Adam** for the same loss function.\n",
        "- Record **execution time** for each optimizer like in previous exercises.\n",
        "- Discuss which combination of **loss + optimizer** converges faster or is more stable.\n"
      ],
      "metadata": {
        "id": "iSDpAnd93gJk"
      }
    }
  ]
}